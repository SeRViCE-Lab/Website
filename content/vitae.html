---
extends: minimal.j2
default_block: main
title: Publications
description: Demos and Talks
---
<!-- Google Anakytics Tracker -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-64680332-3', 'auto');
  ga('send', 'pageview');
  ga('send', 'pageview' '/research.html');
  ga('send', 'pageview' '/speaking.html');
  ga('send', 'pageview'  '/projects.html');
  ga('send', 'pageview' 'http://scriptedonachip.com/');
  ga('send', 'pageview' '/dfwslides');

</script>

  
<html>

<h1>Publications</h1>
<hr/>

<h4><font color="#711E0C">Minimax Iterative Dynamic Game : Application to Nonlinear Robot Control.</font></h4>
    <p><font color="#110D0C">Deep reinforcement learning policies provide good
control decision strategies in high-dimensional state spaces, par- ticularly in complex autonomous tasks. However, they exhibit weak performance guarantees in the presence of disturbance, model mismatch, or model uncertainties. This brittleness limits their use in high-risk scenarios. We present how to quantify the sensitivity of deep reinforcement learning policies â€“ this would inform of its robustness capacity. We then formulate a minimax iterative dynamic game for designing robust policies in the presence of an adversarial input. The algorithm is simple and is adaptable for designing meta-learning/deep policies that are robust against disturbances, model mismatch or model uncertainities, up to a disturbance bound. Videos of our results can be seen <a href="https://goo.gl/JhshTB">here</a>.
 </font><br>
    <font color="blue"><b>Olalekan Ogunmolu*</b>, Nicholas Gans, and Tyler Summers</font>. 
    <br>
    <a href="http://ecs.utdallas.edu/~opo140030/iros18/IROS2018.pdf" class="border"><i class="fa fa-file-pdf-o"></i><b>PDF &nbsp;</b></a> 
    <a href="https://github.com/lakehanne/soft-neuro-adapt"> <b>Code (Github)&nbsp;</b> </a>
    <a href="http://ecs.utdallas.edu/~opo140030/iros18/iros2018.html"> <b>Videos website&nbsp;</b> </a>
  </p>


<h4><font color="#711E0C">Soft-NeuroAdapt: A 3-DOF neuro-adaptive patient pose correction system for frameless and maskless cancer radiotherapy.</font></h4>
    <p><font color="#110D0C">Precise patient positioning is fundamental to successful removal of malignant tumors during treatment of head and neck cancers. Errors in patient positioning have been known to damage critical organs and cause complications. To better address issues of patient positioning and motion, we introduce a 3-DOF neuro-adaptive soft-robot, called Soft-NeuroAdapt to correct deviations along 3 axes. The robot consists of inflatable air bladders that adaptively control head deviations from target while ensuring patient safety and comfort. The adaptive-neuro controller combines a state feedback component, a feedforward regulator, and a neural network that ensures correct adaptation. States are measured by a 3D vision system. We validate Soft-NeuroAdapt on a 3D printed head-and-neck dummy, and demonstrate that the controller provides adaptive actuation that compensates for intrafractional deviations in patient positioning.
 </font><br>
    <font color="blue"><b>Olalekan Ogunmolu*</b>, Adwait Kulkarni, Yonas Tadesse, Xuejun Gu, Steve Jiang, and Nicholas Gans</font>. 
    <br>
    <a href="http://ecs.utdallas.edu/~opo140030/media/Papers/IROS2017.pdf" class="border"><i class="fa fa-file-pdf-o"></i><b>PDF &nbsp;</b></a> 
    <a href="https://arxiv.org/abs/1703.03821v3"><b> arXiv &nbsp;</b></a> 
    <a href="https://github.com/lakehanne/soft-neuro-adapt"> <b>Code (Github)&nbsp;</b> </a>
  </p>

<h4><font color="#711E0C">Nonlinear Systems Identification Using Deep Dynamic Neural Networks.</font></h4>
    <p><font color="#110D0C">An investigation of the effectiveness of deep neural networks in the modeling of dynamical systems with complex behavior. Three deep neural network structures are trained on sequential data, and we investigate the effectiveness of these networks in modeling associated characteristics of the underlying dynamical systems. </font><br>
    <font color="blue"><b>Olalekan Ogunmolu*</b>, Xuejun Gu, Steve Jiang, and Nicholas Gans</font>. <font color="#110D0C">To appear in <i>American Control Conference (ACC)</i>. Seattle, WA. May 2017.</font> <br>
    <a href="{{ media_url('Papers/ACC2016/ACC2016.pdf') }}" class="border"><i class="fa fa-file-pdf-o"></i><b>PDF &nbsp;</b></a> 
    <a href="https://arxiv.org/abs/1610.01439"><b> arXiv &nbsp;</b></a> 
    <a href="https://github.com/lakehanne/FARNN"> <b>Code (Github)&nbsp;</b> </a>
    <a href="http://lakehanne.github.io/Deep-Nets-Identification"> <b>Blog &nbsp;</b> </a>
  </p>


<hr/>
<h4><font color="#711E0C">Vision-based control of a soft-robot for Maskless Cancer Radiotherapy.</font></h4>
    <p><font color="#110D0C">An LQG-based controller for a soft-robot targeted towards accurate patient positioning systems in maskless head and neck cancer radiotherapy. With two RGB-D sensors in a multisensor kalman fusion scenario, we estimate the position of a patient during simulated maskless cancer RT in real-time. </font> <br>
    <font color="blue"><b>Olalekan Ogunmolu*</b>, Xuejun Gu, Steve Jiang, and Nicholas Gans</font>.<font color="#110D0C"><i> IEEE Conference on Automation Science and Engineering (CASE).</i> Fort-Worth, Texas, August 2016. </font><br>
<i class="fa fa-file-pdf-o"></i> <a href="{{ media_url('Papers/CASE2016/CASE2016.pdf') }}" class="border"><b>PDF &nbsp;</b></a> 
<a href="https://arxiv.org/abs/1610.01481"><b> arXiv &nbsp;</b></a> 
<a href="https://github.com/SeRViCE-Lab/RAL-Codes"> <b>Code (Github)</b> &nbsp; </a> 
<a href="dfwslides/CASE2016.html"> <b> Slides</b></a>


<hr/>
<h4><font color="#711E0C">A Real-Time Soft-Robotic Patient Positioning System for Maskless Head-and-Neck Cancer Radiotherapy.</font></h4>
    <p><font color="#110D0C"> Use of inflatable air bladders in controlling the flexion/extension cranial motion of a simulated patient during maskless cancer radiotherapy. We imitate IGRT by using RGB-D depth sensors to generate surface images of a patient and extract the pose information from the reconstructed mesh. Results show that the system is capable of controlling head motion to within 2mm with respect to a reference trajectory. </font> <br>
    <font color="blue"><b>Olalekan Ogunmolu*</b>, Xuejun Gu, Steve Jiang, and Nicholas Gans</font>.<font color="#110D0C"><i> IEEE Conference on Automation Science and Engineering (CASE).</i> Gothenburg, Sweden, August 2015. <br>
    </font>
  <a href="{{ media_url('Papers/CASE2015/CASE2015.pdf') }}" class="border"><b><i class="fa fa-file-pdf-o"></i> PDF &nbsp;</b></a> 
  <a href="https://arxiv.org/abs/1506.04787"><b> arXiv &nbsp;</b></a> 
  <a href="https://github.com/lakehanne/CASE_LabVIEW_Code"> <b>Code (Github)</b> &nbsp; </a> 
  <a href="dfwslides"> <b>Slides&nbsp;</b></a>
  <b>doi: 10.1109/CoASE.2015.7294318 </b>

  <hr/>
  <h4><font color="#711E0C">An Image-Guided Soft Robotic Patient Positioning System for Maskless Head-And-Neck Cancer Radiotherapy: A Proof-Of-Concept Study.</font></h4>
      <p><font color="#110D0C"> Use of inflatable air bladders in controlling the flexion/extension cranial motion of a simulated patient during maskless cancer radiotherapy. We imitate IGRT by using RGB-D depth sensors to generate surface images of a patient and extract the pose information from the reconstructed mesh. Results show that the system is capable of controlling head motion to within 2mm with respect to a reference trajectory. </font> <br>
      <font color="blue"><b>Olalekan Ogunmolu*</b>, Nick Gans, Steve Jiang, Xuejun Gu</font>.<font color="#110D0C"><i> American Association of Physicists in Medicine (AAPM) Annual Meeting.</i> Anaheim, CA, July 2015. <br>
      </font>
    <a href="http://www.aapm.org/meetings/2015AM/PRAbs.asp?mid=99&aid=29621" class="border"><b><i class="fa fa-file-pdf-o"></i> Abstract &nbsp;</b></a> 

<hr/>
<h4><font color="#711E0C">Autonomous Navigation of a Rotor-craft unmanned aerial vehicle
using machine vision.</font></h4>
    <p><font color="#110D0C"> Flying of the ACSE dept quadrotor using extracted image features from a structured indoor environment. We apply invariant moments, extract corners from UAV camera frames and
    compare results with calibrated helipad image; we learn actual helipad features by computing matching variance and then estimate the UAV pose in a hovering condition to plan landing.
</font> <br>
    <font color="blue"><b>Olalekan Ogunmolu.</b></font>.<font color="#110D0C"><i> MS Thesis. ACSE Dept, The University of Sheffield. </i> Sheffield, United Kingdom, August 2012. <br></font>
  <a href="http://lakehanne.github.io/downloads/MS_Thesis.pdf" class="border"><b><i class="fa fa-file-pdf-o"></i> PDF &nbsp;</b></a> 

<br></br>
<hr/>
<h3>Curriculum Vitae</h3>
<p><a href="{{ media_url('pdfs/CV.pdf') }}"><b> Academic CV (.pdf).<br></br>
	<small>Last updated: December, 2016</small></b></a>.</p>
<hr/>

<br/>
<h3>Concise Resume</h3>
<p><a href="{{ media_url('pdfs/Concise_CV.pdf') }}"><b> Resume (.pdf).<br></br>
  <small>Last updated: December, 2016</small></b></a>.</p>
<hr/>
