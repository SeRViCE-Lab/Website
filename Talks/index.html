<!doctype html>
<html lang="en">
<!-- Mathjax script -->
	<head>
		<script type="text/x-mathjax-config">
				MathJax.Hub.Config({
					  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
				});
		</script>
		<!-- <p class="view"><a href="/~opo140030/research.html"> -->
		<script type="text/javascript" src="/~opo140030/dfwslides/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
		</script>

		<!--<script type="text/javascript"
		   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js">
		</script>-->

			<!-- Google Anakytics Tracker -->
		<script>
		  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

		  ga('create', 'UA-64680332-3', 'auto');
		  ga('send', 'pageview');
		  ga('send', 'pageview' '/research.html');
		  ga('send', 'pageview' '/speaking.html');
		  ga('send', 'pageview'  '/projects.html');
		  ga('send', 'pageview' 'http://lakehanne.github.io/');
		  ga('send', 'pageview' '/~opo140030/dfwslides');
		  ga('send', 'pageview' '/dfwslides');

		</script>

		<meta charset="utf-8">

		<title>Towards accurate patient positioning in head and neck cancer radiotherapy. </title>

		<meta name="description" content="A case for automating head and neck cancer radiotherapy treatment">
		<meta name="author" content="Olalekan Ogunmolu">

		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<!-- Replace UTD Balderdash -->
		<link rel="shortcut icon" href="/~opo140030/media/favicon.ico">
		<link rel="apple-touch-icon" href="/~opo140030/media/Pat.jpg">

		<link rel="stylesheet" href="/~opo140030/dfwslides/css/reveal.css">
		<link rel="stylesheet" href="/~opo140030/dfwslides/css/theme/league.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="/~opo140030/dfwslides/lib/css/serif.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? '/~opo140030/dfwslides/css/print/pdf.css' : '/~opo140030/dfwslides/css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
		
	</head>

	<body>
		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
<!-- 
				<section>
					<div id="container"></div>
					<div id="info"><a href="http://threejs.org/examples/webgl_buffergeometry.html" target="_blank" align="middle"><small>Tap the spacebar key to continue</small><br><font size="2">NB: You need webgl to display the graphics on this page</font></a> 
					</div>
				
					<script src="/~opo140030/dfwslides/js/three.min.js"></script>

					<script src="/~opo140030/dfwslides/js/Detector.js"></script>

					<script src="/~opo140030/dfwslides/js/libs/stats.min.js"></script>

					<script>
					
					if ( ! Detector.webgl ) Detector.addGetWebGLMessage();

								var container, stats;

								var camera, scene, renderer;

								var mesh;

								init();
								animate();

								function init() {

									container = document.getElementById( 'container' );

									//

									camera = new THREE.PerspectiveCamera( 27, window.innerWidth / window.innerHeight, 1, 3500 );
									camera.position.z = 2750;

									scene = new THREE.Scene();
									scene.fog = new THREE.Fog( 0x050505, 2000, 3500 );

									//

									scene.add( new THREE.AmbientLight( 0x444444 ) );

									var light1 = new THREE.DirectionalLight( 0xffffff, 0.5 );
									light1.position.set( 1, 1, 1 );
									scene.add( light1 );

									var light2 = new THREE.DirectionalLight( 0xffffff, 1.5 );
									light2.position.set( 0, -1, 0 );
									scene.add( light2 );

									//

									var triangles = 160000;

									var geometry = new THREE.BufferGeometry();

									var positions = new Float32Array( triangles * 3 * 3 );
									var normals = new Float32Array( triangles * 3 * 3 );
									var colors = new Float32Array( triangles * 3 * 3 );

									var color = new THREE.Color();

									var n = 800, n2 = n/2;	// triangles spread in the cube
									var d = 12, d2 = d/2;	// individual triangle size

									var pA = new THREE.Vector3();
									var pB = new THREE.Vector3();
									var pC = new THREE.Vector3();

									var cb = new THREE.Vector3();
									var ab = new THREE.Vector3();

									for ( var i = 0; i < positions.length; i += 9 ) {

										// positions

										var x = Math.random() * n - n2;
										var y = Math.random() * n - n2;
										var z = Math.random() * n - n2;

										var ax = x + Math.random() * d - d2;
										var ay = y + Math.random() * d - d2;
										var az = z + Math.random() * d - d2;

										var bx = x + Math.random() * d - d2;
										var by = y + Math.random() * d - d2;
										var bz = z + Math.random() * d - d2;

										var cx = x + Math.random() * d - d2;
										var cy = y + Math.random() * d - d2;
										var cz = z + Math.random() * d - d2;

										positions[ i ]     = ax;
										positions[ i + 1 ] = ay;
										positions[ i + 2 ] = az;

										positions[ i + 3 ] = bx;
										positions[ i + 4 ] = by;
										positions[ i + 5 ] = bz;

										positions[ i + 6 ] = cx;
										positions[ i + 7 ] = cy;
										positions[ i + 8 ] = cz;

										// flat face normals

										pA.set( ax, ay, az );
										pB.set( bx, by, bz );
										pC.set( cx, cy, cz );

										cb.subVectors( pC, pB );
										ab.subVectors( pA, pB );
										cb.cross( ab );

										cb.normalize();

										var nx = cb.x;
										var ny = cb.y;
										var nz = cb.z;

										normals[ i ]     = nx;
										normals[ i + 1 ] = ny;
										normals[ i + 2 ] = nz;

										normals[ i + 3 ] = nx;
										normals[ i + 4 ] = ny;
										normals[ i + 5 ] = nz;

										normals[ i + 6 ] = nx;
										normals[ i + 7 ] = ny;
										normals[ i + 8 ] = nz;

										// colors

										var vx = ( x / n ) + 0.5;
										var vy = ( y / n ) + 0.5;
										var vz = ( z / n ) + 0.5;

										color.setRGB( vx, vy, vz );

										colors[ i ]     = color.r;
										colors[ i + 1 ] = color.g;
										colors[ i + 2 ] = color.b;

										colors[ i + 3 ] = color.r;
										colors[ i + 4 ] = color.g;
										colors[ i + 5 ] = color.b;

										colors[ i + 6 ] = color.r;
										colors[ i + 7 ] = color.g;
										colors[ i + 8 ] = color.b;

									}

									geometry.addAttribute( 'position', new THREE.BufferAttribute( positions, 3 ) );
									geometry.addAttribute( 'normal', new THREE.BufferAttribute( normals, 3 ) );
									geometry.addAttribute( 'color', new THREE.BufferAttribute( colors, 3 ) );

									geometry.computeBoundingSphere();

									var material = new THREE.MeshPhongMaterial( {
										color: 0xaaaaaa, specular: 0xffffff, shininess: 250,
										side: THREE.DoubleSide, vertexColors: THREE.VertexColors
									} );

									mesh = new THREE.Mesh( geometry, material );
									scene.add( mesh );

									//

									renderer = new THREE.WebGLRenderer( { antialias: false } );
									renderer.setClearColor( scene.fog.color );
									renderer.setPixelRatio( window.devicePixelRatio );
									// renderer.setSize( window.innerWidth, window.innerHeight );
									renderer.setSize( 800, 640 );

									renderer.gammaInput = true;
									renderer.gammaOutput = true;

									container.appendChild( renderer.domElement );

									//

									stats = new Stats();
									stats.domElement.style.position = 'absolute';
									stats.domElement.style.top = '0px';
									container.appendChild( stats.domElement );

									//

									window.addEventListener( 'resize', onWindowResize, false );

								}

								function onWindowResize() {

									camera.aspect = window.innerWidth / window.innerHeight;
									camera.updateProjectionMatrix();

									renderer.setSize( window.innerWidth, window.innerHeight );

								}

								//

								function animate() {

									requestAnimationFrame( animate );

									render();
									stats.update();

								}

								function render() {

									var time = Date.now() * 0.001;

									mesh.rotation.x = time * 0.25;
									mesh.rotation.y = time * 0.5;

									renderer.render( scene, camera );

								}

							</script>				
				</section> -->

				<section >
					<h2>Head and Neck Cancer Radiotherapy</h2>
					<h3>Towards accurate automated immobilization in maskless radiosurgery</h3>
					<p>
						<small>Presented by <a href="http://ecs.utdallas.edu/~olalekan.ogunmolu">Olalekan Ogunmolu</a> / <a href="http://twitter.com/patmeansnoble">@patmeansnoble</a></small>
					</p>
				</section>


				<!-- Example of nested vertical slides -->
				<section data-transition="slide" data-background="#4d7e65" data-background-transition="zoom">
					<section data-background="/~opo140030/dfwslides/images/HNCancerRegions.png" style="display:block;background:#000;opacity:0.4;">
						<h2>Background</h2>
						<a href="#" class="navigate-down">
							<img width="78" height="138" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow" style="display:block;background:#000;opacity:0;">
						</a>
						<br>
					</section>
					<section >
						<p>Head and neck (H&N) cancers are among the most fatal of major cancers in the United States</p>					
						<br>
						<p>2014: 35% of all pharynx and oral cavity cancers developed led to fatility [Siegal, R. et. al]</p>
						<br>
						<p>Cancer kills almost 600,000 people each year in the U.S. alone <br><a href="https://news.developer.nvidia.com/university-of-torontos-gpu-accelerated-cancer-research-wins-nvidia-foundation-award/#sthash.1asn8CaE.g665SP03.dpuf"><small><br>Source: NVIDIA Foundation Award.</small></a></p>
					<!-- 	<br>
						<p>It therefore becomes paramount that treatment be as optimal as possible</p> -->
						<a href="#/2" class="navigate-right">
							<img width="100" height="100" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Right arrow" style="transform: rotate(-90deg); -webkit-transform: rotate(-90deg);">
						</a>
					</section>				
				</section>

				<section>
					<section data-transition="slide"   data-background-transition="zoom">
						<h2>So how are H&N Cancers typically treated?</h2>		
						<ol style="list-style-type:circle">
						  <li>Surgery</li>					  
						  <small>Pros: Oldest technique and most successful</small><br>
						  <small>Cons: Only useful when cancer is localized (highly improbable for most malignant cancers)</small>

						  <li>Chemotherapy</li>
						  <small>Pros: Kills cancer cells throughout the body</small><br>
						  <small>Cons: Highly toxic; can kill healthy cells; highly carcinogenic<br>
						  </small>
						</ol>
						 	<a href="#/2" class="navigate-down">
						 		<img width="178" height="238" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
						 	</a>	
					</section >
					<section  >
						<ol style="list-style-type:circle"> 
						  <li>Radiation Therapy</li><br>
						  <small>Good procedure for distributed cancer cells</small><br>
						  <small> Good palliative treatment when eliminating cancer tumors is impossible</small><br>
						  <small> Helpful in shrinking cancer tumors pre-surgery  or tumor leftovers post-surgery</small><br>
						  <small> Minimal exposure of patient to radiation</small>
						  <small> Often involves a combination of drugs and chemos</small><br>
						  <small> Body cancer radiotherapy (RT) typically use a combination of IMRT & IGRT</small>
						</ol>
						<a href="#/2" class="navigate-right">
							<img width="100" height="100" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Up arrow" style="transform: rotate(-90deg); -webkit-transform: rotate(-90deg);">
						</a>
					</section>
				</section>

				<section>

					<section >
						<h2>IMRT/IGRT, what?</h2><br>	
						<p>IMRT: Intensity Modulated Radiation Therapy</p><br>
						<ul>
							<small><li>Deals with modulating the dosage and shaping of the radiation beam <u>to precise size of tumor cells</u></li></small><br>
							<small><li>IMRT improves accuracy of carefully targeted radiation thereby minimizing exposure of healthy organs</li></small><br>
							<small><li>Deviations still occur between planned dose and delivered dose of radiation</li></small><br>
							<small><li>Enter IGRT</li></small><br>
						</ul>

						<a href="#4/1" class="navigate-down">
							<img width="178" height="180" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
						</a>	
					</section>

					<section >
						<p>IGRT: Image Guidance Radiation Therapy</p><br>
						<ul>
							<small>Deals with precise and accurate patient positioning on a treatment table to avoid dose deviations from planned targets</small>
							<br></br>
							<small>The uncertainty in dose measures to malignant tissues necessitated use of IGRT before treatment</small>
							<br></br>
							<small>Goal was to assure precise localization of the beam onto the target tumor cell</small>
							<br></br>
							<small><a href="http://www.prostatecancercenterpa.com/imrtigrtbenefits.html"><strong>Source</strong>: Prostrate Cancer Center</a></small>
						</ul>

						<a href="#4/2" class="navigate-down">
							<img width="100" height="100" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
						</a>
					</section>

					
					<section >	
							<h3>What frame/mask-based RT entails</h3>
							<p>Accurate markers are placed inside a patient's body after consultation with a medic</p>
							<br>
							<p>Few days afterwards, a radiation-based scan (CT) of the markers is performed to localize the exact position of the markers in the body</p>
							<br>
							<p>The scan provides the size and shape of the cancer cells for computerized treatment planning calculations</p>
						<!-- <br> -->
						<a href="#/2" class="navigate-down">
							<img width="100" height="100" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow" >
						</a>
					</section>

					<section>
						<div class="fig figcenter fighighlight">
						<img src="/~opo140030/dfwslides/images/frame2.png" width="250", height = "250"/>
						<img src="/~opo140030/dfwslides/images/varian.png" width="250", height = "250" />
						<img src="images/cbct.png" width="250", height = "250"/>
						<div class="figcaption" align="middle"><small>(Left) Rigid frame used in frame-based IMRT (Middle) CT Scanner (Right) Linac Couch and scanner</small>
					</div>
						<!-- style="transform: rotate(-90deg); -webkit-transform: rotate(-90deg);" -->
					</section>
					<section  >	
							<p>Current IGRT radiation-based systems include <br><a href="http://ac.els-cdn.com/S0360301613002137/1-s2.0-S0360301613002137-main.pdf?_tid=dd1ca758-9953-11e5-9bd0-00000aab0f6c&acdnat=1449102297_262e84a8d76bc146866d3936a3390fce"><small>[Jennifer De Los Santos et.al., 2012]</small></a></p>
							<br>
							<ul>
								<li>	
										Electronic Portal imaging detectors <br><small>e.g. IGRT and MV imaging; 1 - 2 mm accuracy; does not acquire 3D volumetric info</small>
								</li>
								<br>
								<li>	
										Cone-beam CT <small>retractable conventional x-ray tube and amorphous silicon x-ray detectors mounted either orthogonal to the treatment beam axis; used in lung/throat/liver, brain, head and neck cancer</small>
										<!-- <p><small></small></p> -->
								</li>
								<br>
								<li>	
										Fan-beam CT <small>in-room gantry-moving CT linac system to move across the patient instead of couch moving patient into the scanner as in conventional CT designs</small>
								</li>
							</ul>
							<a href="#/2" class="navigate-down">
							<img width="100" height="100" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow" >
						</a>
					</section>
					<section  >	
							<ul>
								<li>	
										Stereoscopic imaging <br><small>used in CyberKnife; 2D imaging system; accuracy < 1mm</small>
								</li>
								<br>	
								<li>	
										Combination alignment systems: optical imaging and 2-D kV orthogonal imaging<br>
										<small>	Facilitates localization of rigid and mobile targets which may be volumetrically aligned with CBCT</small>
								</li>							
							</ul>
							<br>
							<a href="#/2" class="navigate-right">
							<img width="100" height="100" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Right arrow" style="transform: rotate(-90deg); -webkit-transform: rotate(-90deg);">
						</a>
					</section>
				</section>

				<section data-markdown data-background-transition="zoom">
					###Motivation

					-	Clinical studies have shown that small perturbations cause high sensitivity to IMRT treatment dose <font color="#8A2BE2">[L. Xing, 2000.]</font>

					-	6D couch motion compensaion system is not time-optimal in treatments

							- <small>Treatment is often stopped for a medical physicist to recalibrate patient set-up when there is a deviation from target pose</small>

					- 	Evidence of treatment discomfort and severe pain from long hours of minimally invasive surgery <font color="#8A2BE2">[Takakura, T., et al., 2010]</font>
						<br>
						<a href="#/2" class="navigate-right">
						<img width="100" height="100" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Right arrow" style="transform: rotate(-90deg); -webkit-transform: rotate(-90deg);">
					</a>
				</section>

				<section>
				<section  data-background-transition="zoom">
					<h2>Related Work</h2>
					<ul>
							<li>
								Frameless and Maskless Cranial SRS <font color="purple">[Cervino et. al. 2010]</font>
							</li>
							<br>
							<li>	
								Idea was to verify accuracy of IGRT systems without rigid frames on face 
								<!-- <br><small><p>Frameless and maskless stereoscopic radiosurgery</p></small> -->
							</li>
							<br>
							<li>	
								Employed deformable masks of the following sort:
							</li>	
							<br>
					</ul>								
							
				</section>

				<section data-background="/~opo140030/dfwslides/images/IGRT.png"  >
					<h3><small>Frameless & Maskless radiosurgery</small></h3>
					<br>
					<a href="#/2" class="navigate-down">
						<img width="100" height="100" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow" >
					</a>
				</section>

				<section id="fragments">
					<h3>Pros</h3>
						<ul>	
							<small><li>
								<!-- <p class="fragment grow" > -->
								<p>
								Anthropomorphic head phantoms employed in checking the accuracy of a 3D surface imaging system (AlignRT Vision System)
								</p> 
							</li></small>
							<br>
							<small><li>
								<!-- <p class="fragment grow"> -->
								<p>
								Compared results from an infra-red optical tracking system with the AlignRT vision software system
								</p>
							</li></small>
							<br>
							<small><li>	
								<!-- <p class="fragment grow"> -->
								<p>
								For different couch angles, the difference between phantom positions recorded by the two systems were within 1mm 		displacement  and 1 $\deg$ rotation
								</p>
							</li></small>
							<br>
							<small><li>
								<!-- <p class="fragment grow"> -->
								<p>
								Patient motion due to couch motion was less than 0.2mm
								</p>
							</li></small>
								<!-- <br> -->
								<a href="#/2" class="navigate-down">
								<img width="100" height="100" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
							</a>
						</ul>				
				</section>
				<section>
					<!-- <section id="fragments" > -->
						<h3>Cons</h3>
							<ul>	
								<small><li>
									<!-- <p class="fragment grow" > -->
									<p>
									6DOF positioning systems model the human body as a rigid body
									</p> 
								</li></small>
								<br>
								<small><li>
									<!-- <p class="fragment grow"> -->
									<p>
									No accounting for flexibility/curvature  of neck
									</p>
								</li></small>
								<br>
								<small><li>	
									<!-- <p class="fragment grow"> -->
									<p>
									Limited positioning of patient can reduce effectiveness
									</p>
								</li></small>
								<br>
								
								<small><li>
									<!-- <p class="fragment grow"> -->
									<p>
								If patient moves , therapy must be stopped, patient repositioned, costs time and money
								</p></li>
								</small>
								
									<br>
									<a href="#/2" class="navigate-right">
									<img width="100" height="100" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Right 	arrow" style="transform: rotate(-90deg); -webkit-transform: rotate(-90deg);">
								</a>
							</ul>				
					</section>
				</section>

				<section>
				<section>
					<!-- <h2><a font-color = "blue">Research Goals</a></h2> -->
					<h3><a font-color = "blue"> Research Aims</a></h3>

						<p><small>Accurate and automatic patient positioning system (pre-treatment)</small></p>

						<p><small>In-treatment automatic and accurate patient positioning with patient motion compensation</small></p>

						<h3><a font-color = "blue">Research Objectives</a></h3>
						<p><small>Surface-image control of the cranial flexion/extension motion of a patient during simulated H&N RT (pre-treatment)</small></p>
						<p><small>Use radiation-transparent soft robot system for positioning/manipulation tasks</small> </p>

							<br>
							<a href="#/2" class="navigate-right">
							<img width="100" height="100" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Right 	arrow" style="transform: rotate(-90deg); -webkit-transform: rotate(-90deg);">
						</a>

						<!-- Hacks to swap themes after the page has loaded. Not flexible and only intended for the reveal.js demo deck. -->
		<!-- 				<a href="#" onclick="document.getElementById('theme').setAttribute('href','css/theme/black.css'); return false;">Black (default)</a> -
						<a href="#" onclick="document.getElementById('theme').setAttribute('href','css/theme/white.css'); return false;">White</a> -
						<a href="#" onclick="document.getElementById('theme').setAttribute('href','css/theme/league.css'); return false;">League</a> -
						<a href="#" onclick="document.getElementById('theme').setAttribute('href','css/theme/sky.css'); return false;">Sky</a> -
						<a href="#" onclick="document.getElementById('theme').setAttribute('href','css/theme/beige.css'); return false;">Beige</a> -
						<a href="#" onclick="document.getElementById('theme').setAttribute('href','css/theme/simple.css'); return false;">Simple</a> <br>
						<a href="#" onclick="document.getElementById('theme').setAttribute('href','css/theme/serif.css'); return false;">Serif</a> -
						<a href="#" onclick="document.getElementById('theme').setAttribute('href','css/theme/blood.css'); return false;">Blood</a> -
						<a href="#" onclick="document.getElementById('theme').setAttribute('href','css/theme/night.css'); return false;">Night</a> -
						<a href="#" onclick="document.getElementById('theme').setAttribute('href','css/theme/moon.css'); return false;">Moon</a> -
						<a href="#" onclick="document.getElementById('theme').setAttribute('href','css/theme/solarized.css'); return false;">Solarized</a> 
							</p> -->
				</section>

				<section >	
						<h3><a font-color = "blue">Overview</a></h3>
						<p><small>Initial study and experiment demonstrating a 1-DOF intra-cranial control of patient motion during H&N Cancer RT</small></p>
						<p><small>Testbed is a Mannequin head lying in a supine position on an inflatable air bladder (IAB)</small></p>
						<p><small>Soft-robot consists of the IAB, two 2-port SMC Pnematics Co. 1/4 NPT proportional valves, and silicone tubes for conveying air from a pressurized air compressor</small></p>
						<p><small>An RGB-D camera is employed for head motion sensing and feedback to a control network running on an NI myRIO hardware</small></p>
						<p><small>Work in partnership with Dr. Gans (my advisor) and Drs. Xuejun Gu and Steve Jiang of the Radiation Oncology Department of UT Southwestern, Dallas, TX, USA</small></p>
						<br>
							<a href="#/2" class="navigate-right">
								<img width="100" height="100" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Right 	arrow" style="transform: rotate(-90deg); -webkit-transform: rotate(-90deg);">
							</a>
				</section>

				<section >
					<h3>System Set-up</h3>
					<p>
						<img src="/~opo140030/dfwslides/images/setup.png" />
					</p>
					<a href="#" class="navigate-down">
						<img width="178" height="238" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
					</a>
				</section>
			</section>



				<section>
					
					<!-- <section data-markdown  > -->					
					<section  data-background-transition="zoom">		
						<h3>Vision-based Head Position Estimation</h3>
						<ul>
							<small><li>								
								Kinect RGB-D Camera employed for position-based visual servoing
							</li></small>

							<small><li>
								Considerable depth image to color alignment for rapid prototyping
							</li></small>
						</ul>	

						<h4>Related Work</h4>
						<ul>

							<small><li>
								Real-time Human Pose Recognition in Parts from Single Depth Images.	<small>Jamie Shotton, et. al, CVPR 2011, Best Paper Award</small>
							</li></small><br>

							<small><li>
								Real-time 3D face-tracking based on active appearance model constrained	by depth data. Nikolai Smolyanski et. al, Image and Vision Computing, 2014, MS SDK v1.5.2
							</li></small><br></br>

							<small><li>
								Color lateral resolution of 640 $\times$ 480 pixels at 30 fps with depth resolution of 40 centimeters<br>
								<small>$\approx$ +/5 mm of quantization error</small>
							</li></small>

						</ul>						

							<a href="#" class="navigate-down">
								<img width="78" height="78" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
							</a>
					</section>
					<section data-markdown  >
						-	Active appearance model used in 3D face detection and tracking 
							-	minimize an energy function defined by distance between 3D face model vertices and depth data from a RGBD camera
							-	residual errors used to modify face model during run time
							-	vertices correspond to facial features and can be identified for consistent use
							-	eyes, nose tip and edge of mouth for tracking in image
							-	bridge of nose used for depth in control algorithm
						<a href="#" class="navigate-right">
							<img width="78" height="78" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Right arrow" style="transform: rotate(-90deg); -webkit-transform: rotate(-90deg);">
						</a>
					</section>

				</section>

			<section>
				<section >
					<h3>Face Tracking Procecure</h3>
					<ul>
						<small>
							<li>
								Find a face rectangle within a video frame 
							</li>
						</small><br>

						<small>
							<li>
								Find five points inside the face area - eye centers, mouth corners, and tip of nose
							</li>
						</small><br>

						<small>
							<li>
								Precompute scale of tracked face from the five points un-projected to 3D 
								camera space and scale 3D camera space appropriately.
							</li>
						</small><br></br>

						<small>
							<li>
								Initialize next frame&rsquo;s 2D face shape based on the correspondences
								found by a robust local feature matching between that frame and the previousframe.
							</li><br>
						</small>
					</ul>

							<a href="#" class="navigate-down">
								<img width="78" height="78" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
							</a>
				</section>

				<section >
					####RESULTS
					-	With the depth constrained 2D+3D AAM fitting, we found good position-
						estimation results on a human subject when object is at a distance of 1 to
						2.5m from the Kinect System

					-	Generalization errors and hence incorrect position estimation errors with respect
						to the mannequin head due to inconsistency in depth data

						<a href="#" class="navigate-down">
							<img width="78" height="78" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
						</a>
				</section>

				<section>
					<small><li>
							Xbox is based on the <i>structured light principle</i>:<br>
							<p>::the stereo-matching principle between the infra-red emitter and the optical camera resulted in a baseline offset that caused occlusion shadows on object edges in depth camera</p>

							<p>::therefore occluding contours of objects are not sharply contrasted against background</p>

							<p>::some materials are poor reflectors of IR wavelengths such as non-smooth textured surfaces; therefore drop-out pixels are a must since mannequin has non-sexy hairstyle</p>

							<p>::the power of the IR emitter is limited by safety considerations; means range is constrained to $\approx 4m$</p>

							<aside class="notes" data-markdown>
							       -	The  structured light approach uses the stereo-matching principle between the infra-red emitter and the 	camera, such that the in-built baseline offset resulted in shadows on the side of objects in the depth camera which meant that the occluding contours of objects are not sharply contrasted against background
							       	-	wavelengths of infrared lights were not properly reflected off objects 
							       	-	drop out pixels and hence mesh construction was difficult resulting in non-robust tracking. 
							       	-	The Kinect Xbox also experiences a sizable noise floor which made accurate head position estimation difficult. </li></small>
				</section>

				
				<section >
					<p>
						<div class="fig figcenter fighighlight">
						<img src="/~opo140030/dfwslides/images/metracked.png" width="450", height = "400"/>
						<img src="/~opo140030/dfwslides/images/manikintracked.png" width="450", height = "400" />
						<div class="figcaption" align="middle"><small>(Left) Face Mesh and Tracker (Right) Mesh and Tracker on manikin face.</small>
						</div>
					</p>
				</section>
			</section>

			<section>
				<section >
					<h3>
						<a font color = "blue">Modeling procedure</a>
					</h3>
					<ul>
						<li>
							Collect Data Set, $Z^N$ of input-output signals, at each time step $k$ for a total of $N$ samples
						</li>
						<li>
							Objective: fit a continuous-time parametric model structure similar to a one-step ahead predictor 
							
							<p>$$y^n = -a_{n-1}y^{n-1} - \cdots - a_0 y + b_m u^m +$$</p>
							<p> $$b_{m-1}u^{m-1} + \cdots b_1 \dot{u} + b_0 u$$</p>
						</li>
						<li>					
							Therefore, form the parameter vector  $\theta = [a_{n-1}, \cdots, a_0, b_{m-1}, \cdots, b_0]$  
						</li>
					</ul>
				</section>
				<section >
					<h3>
						<a font color = "blue">Modeling procedure cont'd</a>
					</h3>
					<ul>						
						<li>
							and a memory-fading vector of past input-output data:
							$\phi(t) = [-y^{n-1} \cdots - \dot{y}, - y, u^m \cdots u]$  
							<p>such that the estimated output can be written as </p>
							$$\hat{y}(t|\theta) = \phi^T(t)\theta   $$
						</li>
						<li>
							Choice of excitation signal important to reproduce desired properties of the system in model and avoid wide crests as much as we can
						</li>
						<li>
							Identification Goal: identify the best model, $\chi$, in the set guided by the rigorous freq. distribution analysis
						</li>
					</ul>
				</section>
			</section>

			<section >

				<section data-transition="slide" data-background-transition="zoom"
				data-background="/~opo140030/dfwslides/images/rawhead.jpg" style="opacity:0.4;" width="550", height = "450">
					<!-- background:#000;<h3>Soft robot system model</h3> -->

				</section>
				<section>
					<p>Excitation signal is sawtooth waveform</p>
					<ul>
					<li>
						Integral and differential of a sawtooth waveform preserves the sawtooth
						waveform with only phase and amplitude shifts
					</li>
					

					<li>Spectrum contains both even and odd harmonics of the
						fundamental frequency<br>
						<small>::to excite all frequency dynamics we want from model</small> 
					</li>

					<li>
						Waveform amplitude = 165mA (max. operating current to SMC valves)
					</li>

					<li>
						Frequency was chosen to avoid aliasing [i.e. > 2 $\times$ sampling frequency [Nyquist Sampling theorem] 
					</li> 
					</ul>
				</section>
				<section  >
					<ul>
						<li>
							Excitation signal should produce corresponding rise/decrease in head pitch motion<br>
													<small>:::9,000 samples is not rich enough to avoid inherent noise which dwarfs data structure</small>
						</li>

						<li>
							Therefore, we remove means and linear trends<br>
												<small>::to remove outliers, high frequency spikes etc</small>
						</li>

						<li>
							The rest of the modeling stages is straightforward<br>
												<small>::prewhiten input signals, estimate impulse response (to examine degree of delay in data), examine correlation functions (Wiener model)</small>
						</li>

						<li>
							Crosscorrelation from input to output would tell us about the dynamics of system<br>
												<small>::since it is proportional to the kronecker delta function (impulse response)</small>
						</li>						
					</ul>				
				</section>
				
				<section >
					<h3>
						Cross-Correlation Analysis
					</h3>
					<p>
						The cross-correlation function (between the input and  output) provides an estimate of the system
						impulse response:
					</p>
						\begin{equation}
						\psi_{uy}(\tau) = \dfrac{\sum\limits_{t=\tau+1}^N\left[u(t-\tau) - \bar{u}\right]\left[y(t)-\bar{y}\right]} {\sqrt{\sum\limits_{t=1}^N\left[u(t) - \bar{u}\right]^2}\sqrt{\sum\limits_{t=1}^N\left[y(t) - \bar{y}\right]^2}}
						\end{equation}
					<p>where $\tau = 0, \pm1, \cdots, \pm (N-1)$.</p>
				</section>
				<section >
					<p> The cross-correlation function (CCF) is the convolution of the system
					impulse response and the process auto-correlation function (Wiener-Hopf equation)</p>
						\begin{equation}
							\psi_{uy}(\tau)=\int h(\nu) \mathbb{E}[u(t)u(t+\tau - \nu)]d\nu \\
										   = \int h(\nu) \psi_{uu}(\tau - \nu)d\nu	\label{eq. wiener-hopf}
						\end{equation}
				</section>
				<section >
					<div class="fig figcenter fighighlight">
					  <img src="/~opo140030/dfwslides/images/wiener.png" width="65%" height="250" align="middle" style="border-left: 1px solid black;">  
					  <!-- <img src="images/acf_in.jpg" width="49%" height="250" style="border-left: 1px solid black;"> -->
					  <div class="figcaption" align="middle"><small>Wiener Block-Structured Model <small>(static nonlinearity at output)</small></small>
					  </div>
					</div>
					<ul>
						<small><li>
							CCF between the output and input is proportional to the system impulse response when the input is white noise
						</li></small>

						<small><li>
							Prewhiten input-output signals to change structure of signals
						</li></small>
						<small><li>
							$$U(t) = U_w(t) \dfrac{1}{F(Z^{-1})}$$ where  $U_w(t)$ is a zero mean white input sequence and $F(Z^{-1})$ is an autoregressive model filter 
						</li></small>
					</ul>
				</section>
			</section>
				
				<section >
					<h3>Nonparametric analysis</h3>
					<div class="fig figcenter fighighlight">
					  <img src="/~opo140030/dfwslides/images/ccf_new.jpg" width="49%" height="250" align="middle">  
					  <img src="/~opo140030/dfwslides/images/acf_in.jpg" width="49%" height="250" style="border-left: 1px solid black;">
					  <div class="figcaption" align="right"><small>(Left) Cross-Correlation of input-output signals and (Right) input signal auto-correlation function</small>.
					  </div>
					</div>
				
					<ul>
						<li>
							So we have some information about the system based on the non-parametric analysis. Now what?
						</li>
						<li>
							System has an 18-sample delay ($\approx$ 2 sec delay)
						</li>
					</ul>
				</section>			

		
				<section  >
					<h3>Model Structure Determination</h3>
					<ul>
						<li>To determine the model structure, we used the original detrended data</li>
							<br>

						<li>After shopping around for models, we settled for a linear, second-order grey-box model set whose quality is measurable
							by the mean-square error (MSE)</li>
							<br>
						<li>Choice ensures cost of model is not too high in solving for $\hat{\theta}_N$
						<small>a higher - order complex model is more difficult to use for simulation and control
								design. If it is not marginally better than a simpler model, it may not be worth the higher price [Llung, $\S$ 16.8]</small> </li>
					</ul>
				</section>
				<section > 						
					<h3>Comparing Models</h3>
						<ul>
							<small><li>The confidence interval compares the estimate with the estimated standard deviation from the
							validation dataset</li></small><br></br>
							<small><li>A 95% confidence region (yellow bands) encloses the model response informing us   we  have
							a reliable model [Llung (1999), $\S$ 16.6]</li></small><br></br>
							<small><li>Best fit:</li></small>  
								<small>a second-order process model with delay and a RHP zero:</small><br>
									$$G(s) = \dfrac{-0.0006\left(s-1.7137\right)}{\left(s+0.01\right)\left(s+0.1028\right)}exp^{-2s}.$$    
								<!-- <br> -->
							<small><li>87.35% fit to original data; mean square error of 0.054982 and an Akaikee final prediction error of 1.672.</li></small> 
						</ul>					

				</section>
				<section >
					<section >
						<div class="fig figcenter fighighlight">
						  <img src="/~opo140030/dfwslides/images/bode.png" width="49%" height="250" align="left">  
						  <img src="/~opo140030/dfwslides/images/residuals.png" width="49%" height="250" align="right"> <br>
						  <div class="figcaption" align="middle"><small>(Left). Bode plot of detrended data; (Right) Residuals from input to output</small>
						  </div>
						  <img src="/~opo140030/dfwslides/images/corr.png" width="69%" height="250" style="border-left: 1px solid black;">
						  <div class="figcaption" align="middle"><small>Frequency response plot of residuals to output. </small>
						  </div>
						</div>
					</section>

					<section>
						<h3>Model validation</h3>
						<ul>
							<li>A control system will perform well with an optimal linear sub-model, tolerate disturbances and nonlinearities.</li>
							<br>
							<li>As most of frequency range is noisy, we represent the overall model with  the linear frequency range (using intuition gained from bode analysis)<a href="#/15"> [Slide 15 (Left)] </a> </li>
							<br>
							<li>Correlation analysis of residuals of $y(t)$ from $\hat{y}(t | \hat{\theta}_N)$ tells us if model has picked up important dynamics from system<a href="#/15"> [Slide 15 (Right)] </a> </li>
							<!-- <br> -->
							$$\epsilon(t, \hat{\theta}_N) = y(t) - \hat{y}(t | \hat{\theta}_N)$$
						</ul>
					</section>
					
					<section >
						<ul><li>Prediction errors are computed as a frequency response from the input to residuals <a href="#/15"> [Slide 15 (Bottom)] </a></li></ul>
						<br></br>
						<h3>Control Design</h3>
						<div class="fig figcenter fighighlight">
						  <img src="/~opo140030/dfwslides/images/openloopstep.png" width="69%" height="250" align="middle">  <br>
						  <div class="figcaption" align="middle"><small>Open Loop Step Response of Identified System</small>
						  </div>
						</div>
					</section>				

				</section>

				<section  >
					<section data-transition="slide" data-background-transition="zoom">
						<ul>
							<li>
								System is non-minimum phase with very slow transient response. [<a href="#/15/2">Slide 15/2</a>]							
							</li>
							<li>
								We require a controller that will increase the response time, guarantee closed-loop
								stability whilst balancing robustness and controller aggressiveness.
							</li>
							<li>							
								Approximating the delay with the second-order Pade function,<br>
								\begin{align}
								H(s) & = \dfrac{s^2 - 3s +3}{s^2 + 3s +3},   \label{eq.pade}
								\end{align}
							</li>
						</ul>
					</section>

					<section >
						<ul>
							<li>
								and introducing the PI controller<br>
								\begin{align}
								G_c &= 3.79 + \dfrac{0.0344}{s}  \label{eq.controller}
								\end{align}
							</li>
							<li>
								nested within a PID controller <br>
								\begin{align}
									G_{PID}=3.4993 + \dfrac{0.054765}{s} + 55.8988s,          \label{eq.pd control}
								\end{align}
							</li>
							<li>
								we obtain the following results [<a href="16/2">Slide 16/2 </a>]
							</li>
						</ul>
					</section>
					
					<section >						
						<div class="fig figcenter fighighlight">
						  <img src="/~opo140030/dfwslides/images/Model3.png" width="69%" height="250" align="middle"> <br> 					  
						  <div class="figcaption" align="middle"><small>Block Diagram of Control Network</small><br>				  
						  <img src="/~opo140030/dfwslides/images/PD_Lead_PI.png" width="49%" height="250" align="left">
						  <img src="/~opo140030/dfwslides/images/Step Reference Tracking.png" width="49%" height="250" align="right"> <br>
						  <div class="figcaption" align="left"><small>CL Step Response of Simulated System.</small> &nbsp &nbsb <small>&nbsp &nbsp &nbsp &nbsp &nbsp &nbsp CL Step Reference Trajectory Tracking</small>
						  </div>
						</div>
					</section>

					<section > 
						<div class="fig figcenter fighighlight">
							<img src="/~opo140030/dfwslides/images/final_s.3.png"><br>
							<div class="figcaption" align="middle"><small>Experimental Results: Varying Set-point</small><br>
					  		</div>
						</div>
					</section>

					<section > 
						<div class="fig figcenter fighighlight">
							<img src="/~opo140030/dfwslides/images/final_s.2.png"><br>
							<div class="figcaption" align="middle"><small>Experimental Results: Constant Set-point</small><br>

					  		</div>
						</div>
					</section>
					<!-- <section data-background-video="/~opo140030/dfwslides/videos/CASEvideo.mp4" data-background-color="#000000">
						<small>Note: Head motion in video might be difficult to discern with human eye. See plots on <href=""> previous slide for accurate observations.</small>
					
					</section> -->
				</section>
			<section>
				

				<section data-markdown >
					###Ongoing Work
					-	Extend results to deformable motions of the upper torso, and H&N
					-	Improved bladder control: Optimal LQG Control, Adaptive Control, Dynamic Neural Network control 
						-	(Feedforward of activated neurons + Feedback of weights)
					-	Incorporation of multi-bladders to accommodate multi-axis positioning (Future Work)
					- 	Vicon Mocap System for online monitoring/feedback to controller to avoid kinect quantization errors
				</section>			
			</section>

			<section>

				<section >
					<h3><a font-color="blue">Highlights from recent work</a></h3>
					<p font-color="blue">Objective: minimizing kinect quantization error to obtain better measurements
					<small>[Or "How do you save the day when your NIH funding has not come through?" :D]</small></p>
					<ul>
						<li>
							From statistical estimation and probability theory, we know that averaging multiple observations would reduce uncertainty
						</li><br>

						<li>
							Yeah, we get it. So how does that apply to this work?
						</li><br>

						<li>
							Wed-e-minute! <small>Do I really have to use raw observations? </small>
						</li>
						
						<a href="#" class="navigate-down">
							<img width="78" height="78" data-src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
						</a>
					</ul>
				</section>



				<section>
					<div id="info"><a href="http://threejs.org/examples/#webgl_shaders_ocean" target="_blank">three.js</a> - webgl ocean demo
					</div>

							<script src="/home/eng/o/opo140030/public_html/dfwslides/js/three.min.js"></script>

							<script src="/home/eng/o/opo140030/public_html/dfwslides/js/OrbitControls.js"></script>
							<script src="/home/eng/o/opo140030/public_html/dfwslides/js/Mirror.js"></script>
							<script src="/home/eng/o/opo140030/public_html/dfwslides/js/WaterShader.js"></script>

							<script src="/home/eng/o/opo140030/public_html/dfwslides/js/Detector.js"></script>
							<script src="/home/eng/o/opo140030/public_html/dfwslides/js/libs/stats.min.js"></script>

							<script>

								if ( ! Detector.webgl ) {

									Detector.addGetWebGLMessage();
									document.getElementById( 'container' ).innerHTML = "";

								}

								var container, stats;
								var camera, scene, renderer;
								var sphere;

								var parameters = {
									width: 2000,
									height: 2000,
									widthSegments: 250,
									heightSegments: 250,
									depth: 1500,
									param: 4,
									filterparam: 1
								};

								var waterNormals;

								init();
								animate();

								function init() {

									container = document.createElement( 'div' );
									document.body.appendChild( container );

									renderer = new THREE.WebGLRenderer();
									renderer.setPixelRatio( window.devicePixelRatio );
									renderer.setSize( window.innerWidth, window.innerHeight );
									container.appendChild( renderer.domElement );

									scene = new THREE.Scene();

									camera = new THREE.PerspectiveCamera( 55, window.innerWidth / window.innerHeight, 0.5, 3000000 );
									camera.position.set( 2000, 750, 2000 );

									controls = new THREE.OrbitControls( camera, renderer.domElement );
									controls.enablePan = false;
									controls.minDistance = 1000.0;
									controls.maxDistance = 5000.0;
									controls.maxPolarAngle = Math.PI * 0.495;
									controls.center.set( 0, 500, 0 );

									scene.add( new THREE.AmbientLight( 0x444444 ) );

									var light = new THREE.DirectionalLight( 0xffffbb, 1 );
									light.position.set( - 1, 1, - 1 );
									scene.add( light );


									waterNormals = new THREE.ImageUtils.loadTexture( 'textures/waternormals.jpg' );
									waterNormals.wrapS = waterNormals.wrapT = THREE.RepeatWrapping;

									water = new THREE.Water( renderer, camera, scene, {
										textureWidth: 512,
										textureHeight: 512,
										waterNormals: waterNormals,
										alpha: 	1.0,
										sunDirection: light.position.clone().normalize(),
										sunColor: 0xffffff,
										waterColor: 0x001e0f,
										distortionScale: 50.0,
									} );


									mirrorMesh = new THREE.Mesh(
										new THREE.PlaneBufferGeometry( parameters.width * 500, parameters.height * 500 ),
										water.material
									);

									mirrorMesh.add( water );
									mirrorMesh.rotation.x = - Math.PI * 0.5;
									scene.add( mirrorMesh );


									// load skybox

									var cubeMap = new THREE.CubeTexture( [] );
									cubeMap.format = THREE.RGBFormat;

									var loader = new THREE.ImageLoader();
									loader.load( 'textures/skyboxsun25degtest.png', function ( image ) {

										var getSide = function ( x, y ) {

											var size = 1024;

											var canvas = document.createElement( 'canvas' );
											canvas.width = size;
											canvas.height = size;

											var context = canvas.getContext( '2d' );
											context.drawImage( image, - x * size, - y * size );

											return canvas;

										};

										cubeMap.images[ 0 ] = getSide( 2, 1 ); // px
										cubeMap.images[ 1 ] = getSide( 0, 1 ); // nx
										cubeMap.images[ 2 ] = getSide( 1, 0 ); // py
										cubeMap.images[ 3 ] = getSide( 1, 2 ); // ny
										cubeMap.images[ 4 ] = getSide( 1, 1 ); // pz
										cubeMap.images[ 5 ] = getSide( 3, 1 ); // nz
										cubeMap.needsUpdate = true;

									} );

									var cubeShader = THREE.ShaderLib[ 'cube' ];
									cubeShader.uniforms[ 'tCube' ].value = cubeMap;

									var skyBoxMaterial = new THREE.ShaderMaterial( {
										fragmentShader: cubeShader.fragmentShader,
										vertexShader: cubeShader.vertexShader,
										uniforms: cubeShader.uniforms,
										depthWrite: false,
										side: THREE.BackSide
									} );

									var skyBox = new THREE.Mesh(
										new THREE.BoxGeometry( 1000000, 1000000, 1000000 ),
										skyBoxMaterial
									);

									scene.add( skyBox );


									var geometry = new THREE.IcosahedronGeometry( 400, 4 );

									for ( var i = 0, j = geometry.faces.length; i < j; i ++ ) {

										geometry.faces[ i ].color.setHex( Math.random() * 0xffffff );

									}

									var material = new THREE.MeshPhongMaterial( {
										vertexColors: THREE.FaceColors,
										shininess: 100,
										envMap: cubeMap
									} );

									sphere = new THREE.Mesh( geometry, material );
									scene.add( sphere );

								}

								//

								function animate() {

									requestAnimationFrame( animate );
									render();

								}

								function render() {

									var time = performance.now() * 0.001;

									sphere.position.y = Math.sin( time ) * 500 + 250;
									sphere.rotation.x = time * 0.5;
									sphere.rotation.z = time * 0.51;

									water.material.uniforms.time.value += 1.0 / 60.0;
									controls.update();
									water.render();
									renderer.render( scene, camera );

								}

							</script>
				</section>


				<section>
					<ul>
						<li>							
							Whatever happened to recursive Kalman estimation and multisensor data fusion? 
						</li><br>
						<li>
							Word on the street is people have been using these methods over the past five-some decades to <i>"kill"</i> sensor jitter.
						</li><br>
						
					</ul>
				</section>

			</section>


			<section>
				<section>
					<h3><a font-color="blue">How in the world do you do that?</a></h3>
					<p><b>Easy-peasy, lemon-squeezy!</b></p>
					<ul>
						<small><li>
							Mount multiple RGB-D cameras above manikin head (I used two). One is the Kinect Xbox 360, and the other is the Kinect v1 for Windows sensor
						</li></small><br></br>

						<small><li>
						Both use different electronic perception technologies for inferring displacement  -><br>
						<small>different lateral and range resolutions, and noise characteristics</small>
						</li></small><br></br>

						<small><li>
						Kinect v1 has a higher depth image spatial resolution of 512$\times$424 pixels, compared with the Kinect Xbox Sensor's image spatial resolution of 320 $\times$ 240 pixels 
						</li></small><br></br>

						<small><li>
						Kinect v1 uses temporal and spatial averaging in the neighborhood of $N$ pixels to improve range resolution 
						</li></small><br></br>
						
						<small><li>
							The v1 sensor has a higher depth-map accuracy and lower noise floor exhibiting an auto-covariance of 11.4707$mm^2$ compared with an auto-covariance of 22.7057 $mm^2$ for the Xbox sensor.
						</li></small>

						<aside class="notes">
							The Kinect v2 has a higher depth image spatial resolution of 512$\times$424 pixels at a 30Hz interactive rate, compared with the Kinect Xbox Sensor \cite{soft_robot:ms_constants}'s image spatial resolution of 320 $\times$ 240 pixels at 30Hz. To minimize the noise due to the limited sensor resolution, the Kinect v2 has the following noise improvement capabilities \cite{soft_robot:canesta} are built into it:
							\begin{itemize}
							\item Temporal and spatial averaging in the neighborhood of $N$ pixels are used to improve the range resolution.
							\item Aliasing effect from the `unambiguous distance range' is minimized by taking more than one measurement at different modulation frequencies and computing the least common multiple of the ranges based on the measurement frequencies.
							\item To properly delineate the occlusions on an image's contours, the foreground image is first binarized before the boundaries of the image are extracted through an erosion morphological operation; the eroded image is then gray-level dilated to construct an output range which reduces motion artifacts.
							\item Ambient lighting's effect on the noise floor is minimized by eliminating high-frequency energy-bands in the frequency transformation of every local pixel's neighborhood.
							\item Multiple exposure settings are used to minimize saturation arising from highly reflective objects and non-reflective surfaces.
						</aside>
						</li></small>
					</ul>	
				</section>


				<section>
					<ul>
						<small><li>
							Despite the improved performance of Kinect v1, noise remains an issue as it is the case for every camera-based system.
						</li></small><br></br>

						<small><li>
						To alleviate this, we employ a multisensor data fusion of both Kinect sensors's measurement of the pose of the head
						</li></small><br></br>

						<small><li>
							So we are basically trying to do the same thing as before except that measurement accuracy is more awesome 
						</li></small><br></br>

						<small><li>
						Algorithm: Track features on target's face using Haar cascade classifiers
						</li></small><br></br>

						<small><li>
						Haar classifiers are computationally intensive. So downsample image, and do detection on GPU.
						</li></small>
					</ul>
				</section>


				<section>
					<h3>Face Detection Flowchart </h3>
					<div class="fig figcenter fighighlight">
						<img src="/~opo140030/dfwslides/images/Flowchart.png"  width="69%" height="450" align="middle"><br>
						<!-- <div class="figcaption" align="middle"><small>Control results using fused trackes from two RGB-Ds</small><br>
				  		</div> -->
					</div>

				</section>

				<section>
						<h6>Filtered estimates from sensor observations</h6>
						<div class="fig figcenter fighighlight">
							<img src="/~opo140030/dfwslides/images/kalman_updreal.png"  width="49%" height="350" align="left">
							<img src="/~opo140030/dfwslides/images/Kinect.png"  width="49%" height="350" align="right"><br>
							<div class="figcaption" align="middle"><small>(L) Recursive Kalman smoothing of noisy sensor observation. &nbsp (R) Smoothed Kinect observations with Savitzky-Golay MA Filter.<a href="https://github.com/lakehanne/Savitzky-Golay/blob/master/savgol.cpp"> Code here.</a></small><br>
					  		</div>
						</div>
				</section>
				<section>
						<h6>Results from multisensor fusion.</h6>
						<div class="fig figcenter fighighlight">
							<img src="/~opo140030/dfwslides/images/fusion1.png"  width="49%" height="350" align="left">
							<img src="/~opo140030/dfwslides/images/fusion2.png"  width="49%" height="350" align="right"><br>
							<div class="figcaption" align="middle"><small></small><br>
					  		</div>
						</div>
				</section>

				<section>
					<h6>Head response to test setpoint using fusion of KF posteriors of individual sensor observations</h6>
					<div class="fig figcenter fighighlight">
						<img src="/~opo140030/dfwslides/images/control1.png"  width="49%" height="350" align="left">
						<img src="/~opo140030/dfwslides/images/control2.png"  width="49%" height="350" align="right"><br>
						<div class="figcaption" align="middle"><small>Linear Quadratic Gaussian-based Control using fused tracks from two RGB-Ds</small><br>
				  		</div>
					</div>
				</section >	

				<section>
					<h4>Reference</h4>
					<p>Our arXiv preprints will be out in the first quarter of 2016</p>
					<p>Title: Visual-Servoing Control of a Soft Robot for H&N Cancer Radiotherapy</p>
					<p>Authors: Olalekan Ogunmolu, X. Gu, S. Jiang and N. Gans</p>
				</section>

			</section>


			<section>
				<h3>Conclusions</h3>					
					<ul>
						<li>
							Deviations from desired positions during H&N Cancer RT cause dose variations and degenerate treatment efficacy
						</li>
						<li>
							We have offered a proof-of-concept evaluation and trial of the accurate control of the cranial flexion/extension motion of a patient during maskless H&N RT
						</li>
						<li>
							The soft robot system can track set trajectory within 14 seconds after start-up with the aid of a cascaded control network
						</li>
					</ul>	
			</section>

			<section > 
				<h3>Benefits</h3>
				<ul>
					<li>							
						Comprehensive and accurate control of the patient&rsquo;s position 
					</li>
					<li>
						Elimination of anatomical deformations as a result of positioning error
					</li>
					<li>
						Guarantee of accurate treatment of malignant cancer tissues during IMRT/IGRT RT
					</li>
					<li>
						Eliminate exposure of organs at risk to irradiaion
					</li>
				</ul>
			</section>	

			<section >
				<h3>I like your Slides. Can I have them?</h3>
				<a font-color="blue"><p>No pressure. Simply add <b>"?print-pdf"</b> as a query string to this presentation weblink; then <b>Save as PDF</b><small><br></br><p>Here is an example.</p></small></p></a>
			 	<iframe src="https://www.slideshare.net/slideshow/embed_code/key/KxNqyQq5fcQvUP" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen>  </iframe>
			</section>

			

			<section >
				<h3>References</h3>
				<ul>
					<li>
						Cervino, L. I., et al. Frame-less and mask-less cranial stereotactic radiosurgery: a
						feasibility study. 2010, Physics In Medicine And Biology 55(7): 1863-1873.
					</li><br>
					<li>
						Jemal A, Siegel R, Xu J, Ward E. Cancer statistics, 2010. CA: A Cancer Journal for Clinicians 2010; 60(5):277 - 300.
					</li><br>
					<li>
						L. Llung, System Identification Theory for the User, 2nd Edition, Upper Saddle River,
						NJ, USA. Prentice Hall, 1999.
					</li><br>
				</ul>
			</section>
			<section >
				<h3>References Cont'd</h3>
				<ul>
					<li>
						Xing, L. Dosimetric effects of patient displacement and collimator and gantry angle
						misalignment on intensity modulated radiation therapy. Radiotherapy & Oncology, 2000. 56(1): p. 97 - 108
					</li>
				</ul>
			</section>
			</div>

		</div>

		<script src="/~opo140030/dfwslides/lib/js/head.min.js"></script>
		<script src="/~opo140030/dfwslides/js/reveal.js"></script>

		<script>

			// // Full list of configuration options available at:
			// // https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				// Display the page number of the current slide
				slideNumber: false,

				// Push each slide change to the browser history
				history: true,

				// Enable keyboard shortcuts for navigation
				keyboard: true,
				// Turns fragments on and off globally
				fragments: true,
				// Flags if speaker notes should be visible to all viewers
				// showNotes: false,
				// Number of milliseconds between automatically proceeding to the
				// next slide, disabled when set to 0, this value can be overwritten
				// by using a data-autoslide attribute on your slides
				 autoSlide: 1500,	
				// Stop auto-sliding after user input
				autoSlideStoppable: true,
				// Enable slide navigation via mouse wheel
				// mouseWheel: true
				// // Hides the address bar on mobile devices
				 hideAddressBar: true,

				center: true,

				transition: 'slide', // none/fade/slide/convex/concave/zoom

				multiplex: {
				    // Example values. To generate your own, see the socket.io server instructions.
				    secret: null, // null so the clients do not have control of the master presentation
				    id: '1ea875674b17ca76', // id, obtained from socket.io server
				    url: 'revealjs-51546.onmodulus.net:80' // Location of socket.io server
				},

				// Optional reveal.js plugins
				dependencies: [
					{ src: '/~opo140030/dfwslides/lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: '/~opo140030/dfwslides/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: '/~opo140030/dfwslides/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: '/~opo140030/dfwslides/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: '/~opo140030/dfwslides/plugin/zoom-js/zoom.js', async: true },
					{ src: '/~opo140030/dfwslides/plugin/notes/notes.js', async: true },
        			{ src: '/~opo140030/dfwslides/socket.io/socket.io.js', async: true },
        			{ src: '/~opo140030/dfwslides/plugin/notes-server/client.js', async: true },
        			{ src: '//cdn.socket.io/socket.io-1.3.5.js', async: true },
        			       { src: '/~opo140030/dfwslides/plugin/multiplex/client.js', async: true }
				]
			});
		</script>

	</body>
</html>
